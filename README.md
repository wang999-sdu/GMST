# GMST

GMST is a Python software package used for identifying spatial domains from spatial transcriptome (ST) data.

GMST was developed with Python 3.9 and Pytorch 1.9.0. The specific package versions are provided in requirements.txt. Use the package to conduct marker gene identification analysis with Scanpy 1.8.1.
-----------------------------------------------------------------------
Installation
1. Prepare environment
To install GMST, we recommend using the Anaconda Python Distribution and creating an isolated environment, so that the GMST and dependencies don't conflict or interfere with other packages or applications. To create the environment, run the following script in command line:

conda create -n GMST_env python=3.9

After create the environment, you can activate the GMST_env environment by:

conda activate GMST_env

2. Install Pytorch
Please install Pytorch that match your machine and environment first by following the instructions on : https://pytorch.org/get-started/locally/

Note that if you want to install Pytorch on a GPU machine, you need to install CUDA first, see guide here for installing CUDA https://developer.nvidia.com/cuda-downloads.

3. Install GMST

git clone https://github.com/
cd GMST/
----------------------------------------------------------------------------
Datasets

The experiment datasets used in this article are available at the following links. 
(1) Human DLPFC 10X Visium data at [http://research.libd.org/spatialLIBD/]; 
(2) human breast cancer 10X Visium data: Invasive Ductal Carcinoma Stained With Fluorescent CD3 Antibody at [https://support.10xgenomics.com/spatial-geneexpression/datasets]; 
(3) The mouse olfactory bulb tissue data generated by 10X Visium, Stereo-seq and Slide-seqV2 platforms can be accessed from [https://www.10xgenomics.com/resources/datasets; https://github.com/JinmiaoChenLab/SEDR_analyses] and [https://singlecell.broadinstitute.org/single_cell/study/SCP815]; 
(4) Slide-seq data: [https://singlecell.broadinstitute.org/single_cell/study/SCP815/sensitive-spatial-genome-wide-expression-profiling-at-cellular-resolution#study-summary]; 
(5) The Colorectal Cancer Liver metastases datasets is sourced from the mentioned article[49] [http://www.cancerdiversity.asia/scCRLM] [http://www.cancerdiversity.asia/scCRLM]; 
(6) Mouse hippocampus and cerebellum Slide-seq data: [https://singlecell.broadinstitute.org/single_cell/study/SCP815/sensitive-spatial-genome-wide-expression-profiling-at-cellular-resolution#study-summary]; 
(7) The processed SeqFISH mouse embryogenesis dataset with segmentation information and associate metadata are available at [https://crukci.shinyapps.io/SpatialMouseAtlas/] 
(8) The mouse embryo data can be accessed at [https://db.cngb.org/stomics/mosta/] 
(9) osmFISH data: [http://linnarssonlab.org/osmFISH/]; 
(10) ExSeq data: [10.5281/zenodo.4075515]; 
(11) The STARmap data was obtained from the mentioned article [DOI: 10.1126/science.aat5691.]; 
(12) Mouse brain STARmap data at [https:// www.starmapresources.org/data]; 
(13) BaristaSeq Visual cortex data: [https://spacetx.github.io/data.html]; 
(14) STARmapPLUS data: [https://singlecell.broadinstitute.org/single_cell/study/SCP1375]; (15) Allen Reference Atlas: [https://mouse.brain-map.org/experiment/thumbnails/100048576?image_type=atlas]. Source data are provided with this paper.

---------------------------------------------------------------------------
Usage

from data_loading import load_data
from preprocessing import preprocess_data
from models import MaskedAutoencoder
from training import train_model
from evaluation import evaluate_model
from visualization import visualize_clusters, paga_analysis, umap_visualization
from config import device, hidden_dim, num_clusters

# Data input
data_path = "/home/wangwm/GFT_VAE/20241017/#1ST_data/0.DLPFC/151507"
image_path = f"{data_path}/spatial/tissue_hires_image.png"
scalefactors_path = f"{data_path}/spatial/scalefactors_json.json"


def main():
    # Load data
    adata = load_data()

    # Data processing
    data_tensor, adj_matrix_tensor, adata = preprocess_data(adata)

    model = MaskedAutoencoder(data_tensor.shape[1], hidden_dim).to(device)
    model.apply(weights_init)

    # Train model
    train_model(model, data_tensor, adj_matrix_tensor)

    # Encoder
    with torch.no_grad():
        gcn_output = model.gcn(data_tensor, data_tensor, adj_matrix_tensor)
        encoded_data = model.fc_encoder(gcn_output)
        encoded_data_np = encoded_data.cpu().numpy()

    # Model evaluation
    ari, nmi, fmi, moran, geary = evaluate_model(encoded_data_np, adata, num_clusters)

    # Result visualization
    visualize_clusters(adata, clusters, image_path, scalefactors_path)
    paga_analysis(adata)
    umap_visualization(adata)

if __name__ == "__main__":
    main()
